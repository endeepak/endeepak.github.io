<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: devops | !geek]]></title>
  <link href="http://tech.endeepak.com/blog/categories/devops/atom.xml" rel="self"/>
  <link href="http://tech.endeepak.com/"/>
  <updated>2019-02-14T06:52:00+05:30</updated>
  <id>http://tech.endeepak.com/</id>
  <author>
    <name><![CDATA[Deepak Narayana Rao]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Monitoring DB backups using prometheus]]></title>
    <link href="http://tech.endeepak.com/blog/2017/12/22/monitoring-db-backups-using-prometheus"/>
    <updated>2017-12-22T14:19:02+05:30</updated>
    <id>http://tech.endeepak.com/blog/2017/12/22/monitoring-db-backups-using-prometheus</id>
    <content type="html"><![CDATA[<p>Ensuring correct database backups taken at regular interval is very critical for disaster recovery. Recent <a href="https://about.gitlab.com/2017/02/01/gitlab-dot-com-database-incident/">gitlab database incident</a> re-emphasizes this fact. <a href="https://about.gitlab.com/">Gitlab</a> was very transparent about this and <a href="https://gitlab.com/gitlab-com/www-gitlab-com/issues/1108">documented</a> approaches for preventing these failures</p>

<p>The preventive measures include, monitoring -</p>

<ul>
<li><em>Backup file is created in every x interval</em>: Catches backups not being uploaded due to backup script error or scheduling error</li>
<li><em>Size of latest backup file is at-least y bytes</em>: Catches erroneous backup file uploaded due to script error</li>
</ul>


<!-- More -->


<p>In our case, DB backups are uploaded to Azure blob storage(similar to AWS S3) and <a href="https://prometheus.io/">prometheus</a> is used for monitoring</p>

<p>High level design</p>

<ul>
<li>Run an exporter which can expose metrics such as <code>latest_file_timestamp</code> and <code>latest_file_size</code> for each blob container where backup files are uploaded</li>
<li>Alert if <code>current_time - latest_file_timestamp &gt; backup_interval</code> or <code>latest_file_size &lt; expected_backup_file_size</code></li>
</ul>


<p>As we couldn&rsquo;t find any existing exporter, we wrote <a href="https://github.com/project-sunbird/prometheus-azure-blob-exporter">prometheus-azure-blob-exporter</a> to capture following metrics</p>

<pre><code># Last modified timestamp(milliseconds) for latest file in container
azure_blob_latest_file_timestamp{container="postgresql-backup"} 1508707802000.0
# Size in bytes for latest file in container
azure_blob_latest_file_size{container="postgresql-backup"} 5606443.0
</code></pre>

<p>Alerts are defined as</p>

<ul>
<li>Check backup is created every day</li>
</ul>


<pre><code># 24 hours + 1 hour slack time for backup process
# threshold_interval_in_milliseconds =&gt; 25 * 60 * 60 * 1000 =&gt; 90000000
ALERT backup_is_too_old
  IF (time() * 1000) - azure_blob_latest_file_timestamp &gt; 90000000
  FOR 5m
  ANNOTATIONS {
      summary = "Backup is too old",
      description = "There is no backup file created for a day in ",
  }
</code></pre>

<ul>
<li>Check latest backup file created has minimum size of 1MB</li>
</ul>


<pre><code># threshold_size_in_bytes =&gt; 1MB =~ 1000000
ALERT backup_size_is_too_small
  IF azure_blob_latest_file_size &lt; 1000000
  FOR 5m
  ANNOTATIONS {
      summary = "Backup size is too small",
      description = "Latest backup file is smaller than 1MB in ",
  }
</code></pre>

<p>Please checkout the <a href="https://github.com/project-sunbird/prometheus-azure-blob-exporter">github repo</a> for more details</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[To cron or not to cron]]></title>
    <link href="http://tech.endeepak.com/blog/2017/12/22/to-cron-or-not-to-cron"/>
    <updated>2017-12-22T12:16:01+05:30</updated>
    <id>http://tech.endeepak.com/blog/2017/12/22/to-cron-or-not-to-cron</id>
    <content type="html"><![CDATA[<p>Using crontab for scheduling background jobs like database backup, purging old data is a common practice in lot of IT organizations. As these jobs are running in background, failures can get missed easily without proper monitoring. In most cases, failures occur due to an error in job script and some times due to mistakes in cron configuration</p>

<!-- More -->


<p>Monitoring short lived cron jobs are not straight forward compared to monitoring long running services like web services. These are few well-known mechanisms for alerting cron job failures</p>

<ul>
<li>Cron job writes to a file on success. Monitoring system regularly reads this file and alerts based on last modified time-stamp of the file -[<a href="https://serverfault.com/questions/33145/techniques-to-monitor-cron-tasks">Refer</a>]</li>
<li>Cron job pings a <a href="https://deadmanssnitch.com/">monitoring service</a> on success. This monitoring service acts as a <a href="https://en.wikipedia.org/wiki/Dead_man%27s_switch">dead man&rsquo;s switch</a> and alerts if there was no call made by the job within expected time interval</li>
</ul>


<h3>Pragmatic alternative for crontab</h3>

<p>If you have a CI server like jenkins in your infrastructure, one of the approach that has worked well for us is - create a scheduled job in jenkins with slack/email notifications on failures.</p>

<p>Advantages of this approach</p>

<ul>
<li>Simpler to setup and effective for catching job script failures</li>
<li>UI to check job output and history of previous runs</li>
<li>Ability to manually trigger the job when needed</li>
</ul>


<h3>Important note</h3>

<p>Whether you use scheduled jobs in crontab or jenkins, you shouldn&rsquo;t depend only on the job&rsquo;s exit status for determining success - [<a href="https://about.gitlab.com/2017/02/01/gitlab-dot-com-database-incident/">Refer</a>]. It is important to have alerting based on expected state of the system after job execution. <em>Example</em>: Timestamp of latest backup file uploaded to backup storage, minimum size of the backup file etc</p>

<p><strong>UPDATE</strong>: Checkout <a href="https://tech.endeepak.com/blog/2017/12/22/monitoring-db-backups-using-prometheus/">Monitoring DB backups using prometheus</a> for more details on monitoring DB backups</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Docker swarm in production]]></title>
    <link href="http://tech.endeepak.com/blog/2017/12/22/docker-swarm-in-production"/>
    <updated>2017-12-22T09:58:33+05:30</updated>
    <id>http://tech.endeepak.com/blog/2017/12/22/docker-swarm-in-production</id>
    <content type="html"><![CDATA[<p>This is a experience report on using docker swarm(17.06) in production for ~3 months</p>

<h3>Context</h3>

<p>Prior to this, we were deploying services on VMs using ansible. For a new project, we wanted to explore benefits of running service as containers and decided to use docker swarm due to its simpler setup and consistency with docker engine APIs</p>

<!-- More -->


<h3>Benefits</h3>

<p>Using container orchestration engine to run services provided following benefits</p>

<ul>
<li>Cleaner boundary between Developers building/packaging the services and DevOps team providing platform for running these services. It is a great cultural shift for the organization!</li>
<li>Consistent way to operate services - start/stop, find logs, scale. This makes it easier to operate</li>
<li>Implementing cross cutting concerns like log aggregation, monitoring gets easier due to consistency</li>
<li>Promotes stateless &amp; immutable infrastructure. It is easier to setup new environments &amp; tear down as and when needed</li>
<li>You can achieve better resource utilization with same level of isolation compared to running services on VMs. Helps overcoming resource management issues when running services in public cloud VMs - which doesn&rsquo;t allow fine gained control on customizing memory, CPU and network resources for a service</li>
</ul>


<p>Docker swarm specific</p>

<ul>
<li>Easy to setup &amp; operate - single binary/service has all the bells and whistles</li>
<li>Smaller learning curve due to API consistency with the standalone docker engine</li>
<li>Built in <a href="https://docs.docker.com/engine/swarm/networking/#configure-service-discovery">service discovery</a>(via DNS)</li>
<li>Built in <a href="https://docs.docker.com/engine/swarm/ingress/">load balancing</a> across service replicas using battle tested Linux services like IPVS. Load balancing is based on health of the instance determined by <a href="https://docs.docker.com/compose/compose-file/#healthcheck">health checks</a> defined for the service</li>
<li>Support for distributed <a href="https://docs.docker.com/engine/swarm/configs/">configuration</a> and <a href="https://docs.docker.com/engine/swarm/secrets/">secrets</a></li>
<li>Support for <a href="https://docs.docker.com/compose/compose-file/#update_config">rolling upgrades</a></li>
</ul>


<h3>Challenges</h3>

<ul>
<li>Sometimes you&rsquo;ll run into issues <a href="https://github.com/moby/moby/issues/34163">moby#34163</a>, <a href="https://github.com/moby/moby/issues/25432">moby#25432</a> during deployments. Having a <a href="https://github.com/project-sunbird/sunbird-devops/wiki/Docker-swarm-troubleshooting">play-book</a> will help in faster recovery from these failures</li>
<li>Issues in overlay networking(<a href="https://github.com/docker/swarm/issues/2161">swarm#2161</a>) can make few containers unreachable some times. Having good monitoring is essential for identification and remediation of these issues</li>
<li>Rolling update is harder to achieve if you are using docker config/secret. Swarm doesn&rsquo;t support updating config/secret. You would have create new config(with new version number) &amp; update the service to use new version of config. It would have been good if this was handled internally using checksum of the config</li>
</ul>


<h3>Conclusion</h3>

<p>Overall, using a container orchestration engine in production has proven to be very productive and useful. Docker swarm is maturing over time, with improved stability it is a promising platform for running containers in production</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Evolution of monitoring systems]]></title>
    <link href="http://tech.endeepak.com/blog/2017/10/21/evolution-of-monitoring-systems"/>
    <updated>2017-10-21T19:54:38+05:30</updated>
    <id>http://tech.endeepak.com/blog/2017/10/21/evolution-of-monitoring-systems</id>
    <content type="html"><![CDATA[<p>Monitoring systems have evolved over time to support evolving architecture styles and deployment strategies</p>

<h2>Generation X+1: Monolithic applications, Bare metal servers</h2>

<p>This was the era where you would have single deployable unit for whole application (labeled as monolithic now). Application and the database would be deployed on a known set of bare metal servers. Vertical scaling was more favorable option for scaling</p>

<!-- More -->


<p><a href="https://www.nagios.org">Nagios</a> was(probably still is) the widely used open source monitoring software in this era. You would configure a list of known severs the monitoring system needs to probe to determine the health of the system. This system was simple to configure &amp; operate</p>

<p><img class="center" src="/images/evolution-of-monitoring-systems/nagios-architecture.png"></p>

<p><em>Image Credit:</em> <a href="https://support.nagios.com/kb/article.php?id=141">https://support.nagios.com/kb/article.php?id=141</a></p>

<h2>Generation X+2: Service Oriented Architecture, VMs on the Cloud</h2>

<p>Managing monolithic applications became painful as the businesses grew. Service Oriented Architecture(SOA) became mainstream in this era. Cloud services like AWS made it very easy to launch new VMs for deploying services. Configuration management tools like puppet, ansible made it easy to deploy applications across large number of servers. <a href="https://martinfowler.com/bliki/ImmutableServer.html">Immutable server pattern</a> was evangelized by <a href="https://medium.com/netflix-techblog/how-we-build-code-at-netflix-c5d9bd727f15">well known companies</a>.</p>

<p>Horizontal scalability started becoming more favorable option for scaling. One could bring up new instances of service by launching VMs from a image(eg: AMIs) and bring down VMs easily depending on the load on the system. This deployment architecture demanded a monitoring system which can handle this dynamicity.</p>

<p>Monitoring systems like <a href="https://sensuapp.org/">Sensu</a> solved this issue with a different architecture style. Instead of a central server probing a static list of servers, sensu had a publish &amp; subscription model using RabbitMQ. The monitoring agent running on the application server subscribes to monitoring check messages relevant for the service and pushes the results via same messaging system. Monitoring server itself was horizontally scalable to handle varying load</p>

<p><img class="center" src="/images/evolution-of-monitoring-systems/sensu-architecture.gif"></p>

<p><em>Image Credit</em>: <a href="https://sensuapp.org/docs/1.0/overview/architecture.html">https://sensuapp.org/docs/1.0/overview/architecture.html</a></p>

<h2>Generation X+3: Micro services, Container orchestration engines</h2>

<p>Micro services are becoming mainstream now. Deploying stateless services as containers has made the concept of immutable servers very easy and efficient compared to using VMs. Container orchestration engines like kubernetes, docker swarm have made it easy to run and manage large number of services running as containers inside a cluster of servers. These orchestration engines also provide benefits like auto healing(restart on failure), easy to scale, in-built service discovery &amp; load balancing.</p>

<p>Monitoring systems like sensu doesn&rsquo;t fit well in this setup. Running a monitoring agent alongside the application process in a container adds <a href="https://devops.stackexchange.com/questions/447/why-it-is-recommended-to-run-only-one-process-in-a-container">additional complexity</a> in containers. As service discovery is provided by container orchestration engines, there is no need for adding complexity of running messaging system for solving discovery problem</p>

<p>During the time when Google open sourced kubernetes(most popular container orchestration engine as of now), a new monitoring system - <a href="https://prometheus.io">Prometheus</a> (built by ex-googlers in soundcloud) started gaining lot of traction. Prometheus leverages service discovery mechanisms for registering services to be monitored. It has a lot simpler setup and smaller resource requirements compared to system like sensu. Prometheus factored the containers ecosystem and fits very well for the job. The scalability argument against pull model of monitoring was also <a href="https://prometheus.io/blog/2016/07/23/pull-does-not-scale-or-does-it/">addressed</a> by the authors of the system.</p>

<p><img class="center" src="/images/evolution-of-monitoring-systems/prometheus-architecture.svg"></p>

<p><em>Image Credit</em>: <a href="https://prometheus.io/docs/introduction/overview/">https://prometheus.io/docs/introduction/overview/</a></p>

<h2>Conclusion</h2>

<p>If you are building a new system with architecture patterns and deployment strategies of this era, <a href="https://prometheus.io">Prometheus</a> is a leading choice among open source monitoring systems</p>

<p><em>PS</em>: There are lot of good things(and few limitations) about prometheus which deserves separate blog post :)</p>
]]></content>
  </entry>
  
</feed>
