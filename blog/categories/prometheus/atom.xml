<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: prometheus | !geek]]></title>
  <link href="https://tech.endeepak.com/blog/categories/prometheus/atom.xml" rel="self"/>
  <link href="https://tech.endeepak.com/"/>
  <updated>2021-09-22T09:46:52+05:30</updated>
  <id>https://tech.endeepak.com/</id>
  <author>
    <name><![CDATA[Deepak Narayana Rao]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Monitoring DB backups using prometheus]]></title>
    <link href="https://tech.endeepak.com/blog/2017/12/22/monitoring-db-backups-using-prometheus"/>
    <updated>2017-12-22T14:19:02+05:30</updated>
    <id>https://tech.endeepak.com/blog/2017/12/22/monitoring-db-backups-using-prometheus</id>
    <content type="html"><![CDATA[<p>Ensuring correct database backups taken at regular interval is very critical for disaster recovery. Recent <a href="https://about.gitlab.com/2017/02/01/gitlab-dot-com-database-incident/">gitlab database incident</a> re-emphasizes this fact. <a href="https://about.gitlab.com/">Gitlab</a> was very transparent about this and <a href="https://gitlab.com/gitlab-com/www-gitlab-com/issues/1108">documented</a> approaches for preventing these failures</p>

<p>The preventive measures include, monitoring -</p>

<ul>
<li><em>Backup file is created in every x interval</em>: Catches backups not being uploaded due to backup script error or scheduling error</li>
<li><em>Size of latest backup file is at-least y bytes</em>: Catches erroneous backup file uploaded due to script error</li>
</ul>


<!-- More -->


<p>In our case, DB backups are uploaded to Azure blob storage(similar to AWS S3) and <a href="https://prometheus.io/">prometheus</a> is used for monitoring</p>

<p>High level design</p>

<ul>
<li>Run an exporter which can expose metrics such as <code>latest_file_timestamp</code> and <code>latest_file_size</code> for each blob container where backup files are uploaded</li>
<li>Alert if <code>current_time - latest_file_timestamp &gt; backup_interval</code> or <code>latest_file_size &lt; expected_backup_file_size</code></li>
</ul>


<p>As we couldn&rsquo;t find any existing exporter, we wrote <a href="https://github.com/project-sunbird/prometheus-azure-blob-exporter">prometheus-azure-blob-exporter</a> to capture following metrics</p>

<pre><code># Last modified timestamp(milliseconds) for latest file in container
azure_blob_latest_file_timestamp{container="postgresql-backup"} 1508707802000.0
# Size in bytes for latest file in container
azure_blob_latest_file_size{container="postgresql-backup"} 5606443.0
</code></pre>

<p>Alerts are defined as</p>

<ul>
<li>Check backup is created every day</li>
</ul>


<pre><code># 24 hours + 1 hour slack time for backup process
# threshold_interval_in_milliseconds =&gt; 25 * 60 * 60 * 1000 =&gt; 90000000
ALERT backup_is_too_old
  IF (time() * 1000) - azure_blob_latest_file_timestamp &gt; 90000000
  FOR 5m
  ANNOTATIONS {
      summary = "Backup is too old",
      description = "There is no backup file created for a day in ",
  }
</code></pre>

<ul>
<li>Check latest backup file created has minimum size of 1MB</li>
</ul>


<pre><code># threshold_size_in_bytes =&gt; 1MB =~ 1000000
ALERT backup_size_is_too_small
  IF azure_blob_latest_file_size &lt; 1000000
  FOR 5m
  ANNOTATIONS {
      summary = "Backup size is too small",
      description = "Latest backup file is smaller than 1MB in ",
  }
</code></pre>

<p>Please checkout the <a href="https://github.com/project-sunbird/prometheus-azure-blob-exporter">github repo</a> for more details</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Evolution of monitoring systems]]></title>
    <link href="https://tech.endeepak.com/blog/2017/10/21/evolution-of-monitoring-systems"/>
    <updated>2017-10-21T19:54:38+05:30</updated>
    <id>https://tech.endeepak.com/blog/2017/10/21/evolution-of-monitoring-systems</id>
    <content type="html"><![CDATA[<p>Monitoring systems have evolved over time to support evolving architecture styles and deployment strategies</p>

<h2>Generation X+1: Monolithic applications, Bare metal servers</h2>

<p>This was the era where you would have single deployable unit for whole application (labeled as monolithic now). Application and the database would be deployed on a known set of bare metal servers. Vertical scaling was more favorable option for scaling</p>

<!-- More -->


<p><a href="https://www.nagios.org">Nagios</a> was(probably still is) the widely used open source monitoring software in this era. You would configure a list of known severs the monitoring system needs to probe to determine the health of the system. This system was simple to configure &amp; operate</p>

<p><img class="center" src="/images/evolution-of-monitoring-systems/nagios-architecture.png"></p>

<p><em>Image Credit:</em> <a href="https://support.nagios.com/kb/article.php?id=141">https://support.nagios.com/kb/article.php?id=141</a></p>

<h2>Generation X+2: Service Oriented Architecture, VMs on the Cloud</h2>

<p>Managing monolithic applications became painful as the businesses grew. Service Oriented Architecture(SOA) became mainstream in this era. Cloud services like AWS made it very easy to launch new VMs for deploying services. Configuration management tools like puppet, ansible made it easy to deploy applications across large number of servers. <a href="https://martinfowler.com/bliki/ImmutableServer.html">Immutable server pattern</a> was evangelized by <a href="https://medium.com/netflix-techblog/how-we-build-code-at-netflix-c5d9bd727f15">well known companies</a>.</p>

<p>Horizontal scalability started becoming more favorable option for scaling. One could bring up new instances of service by launching VMs from a image(eg: AMIs) and bring down VMs easily depending on the load on the system. This deployment architecture demanded a monitoring system which can handle this dynamicity.</p>

<p>Monitoring systems like <a href="https://sensuapp.org/">Sensu</a> solved this issue with a different architecture style. Instead of a central server probing a static list of servers, sensu had a publish &amp; subscription model using RabbitMQ. The monitoring agent running on the application server subscribes to monitoring check messages relevant for the service and pushes the results via same messaging system. Monitoring server itself was horizontally scalable to handle varying load</p>

<p><img class="center" src="/images/evolution-of-monitoring-systems/sensu-architecture.gif"></p>

<p><em>Image Credit</em>: <a href="https://sensuapp.org/docs/1.0/overview/architecture.html">https://sensuapp.org/docs/1.0/overview/architecture.html</a></p>

<h2>Generation X+3: Micro services, Container orchestration engines</h2>

<p>Micro services are becoming mainstream now. Deploying stateless services as containers has made the concept of immutable servers very easy and efficient compared to using VMs. Container orchestration engines like kubernetes, docker swarm have made it easy to run and manage large number of services running as containers inside a cluster of servers. These orchestration engines also provide benefits like auto healing(restart on failure), easy to scale, in-built service discovery &amp; load balancing.</p>

<p>Monitoring systems like sensu doesn&rsquo;t fit well in this setup. Running a monitoring agent alongside the application process in a container adds <a href="https://devops.stackexchange.com/questions/447/why-it-is-recommended-to-run-only-one-process-in-a-container">additional complexity</a> in containers. As service discovery is provided by container orchestration engines, there is no need for adding complexity of running messaging system for solving discovery problem</p>

<p>During the time when Google open sourced kubernetes(most popular container orchestration engine as of now), a new monitoring system - <a href="https://prometheus.io">Prometheus</a> (built by ex-googlers in soundcloud) started gaining lot of traction. Prometheus leverages service discovery mechanisms for registering services to be monitored. It has a lot simpler setup and smaller resource requirements compared to system like sensu. Prometheus factored the containers ecosystem and fits very well for the job. The scalability argument against pull model of monitoring was also <a href="https://prometheus.io/blog/2016/07/23/pull-does-not-scale-or-does-it/">addressed</a> by the authors of the system.</p>

<p><img class="center" src="/images/evolution-of-monitoring-systems/prometheus-architecture.svg"></p>

<p><em>Image Credit</em>: <a href="https://prometheus.io/docs/introduction/overview/">https://prometheus.io/docs/introduction/overview/</a></p>

<h2>Conclusion</h2>

<p>If you are building a new system with architecture patterns and deployment strategies of this era, <a href="https://prometheus.io">Prometheus</a> is a leading choice among open source monitoring systems</p>

<p><em>PS</em>: There are lot of good things(and few limitations) about prometheus which deserves separate blog post :)</p>
]]></content>
  </entry>
  
</feed>
